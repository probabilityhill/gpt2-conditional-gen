{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"conditional_text_generation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPnk5omTxsTH/xzDh5Q5DJI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"ddf5687c4b6c4c72b301f9a524244012":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_47ef2c820ef64d609d44c4d6dadf4423","IPY_MODEL_2765c37a923143bea2cccfa5591f80d9","IPY_MODEL_cd6287e886684f4a81ad47c9a3b4fd26"],"layout":"IPY_MODEL_d5b96c1c7554417683894ebe862fffc7"}},"47ef2c820ef64d609d44c4d6dadf4423":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_927d4d95519e4d42a2f6eaa2cf38e344","placeholder":"​","style":"IPY_MODEL_b992f7b118404e479de1294984434619","value":"Downloading: 100%"}},"2765c37a923143bea2cccfa5591f80d9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3b082b4521e4cbd851a0ca9df585e80","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e615d5805e96415fa2f0c6ade6cc59d0","value":665}},"cd6287e886684f4a81ad47c9a3b4fd26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fab742fd8f154d9db9e8282af795af97","placeholder":"​","style":"IPY_MODEL_79345e556f2e412b9e25efa72fbd30c7","value":" 665/665 [00:00&lt;00:00, 20.7kB/s]"}},"d5b96c1c7554417683894ebe862fffc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"927d4d95519e4d42a2f6eaa2cf38e344":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b992f7b118404e479de1294984434619":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3b082b4521e4cbd851a0ca9df585e80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e615d5805e96415fa2f0c6ade6cc59d0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fab742fd8f154d9db9e8282af795af97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79345e556f2e412b9e25efa72fbd30c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34a146a132e34475bc20442d02240c78":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_85b7c351ee694dae86d71d4e8c267311","IPY_MODEL_b1724dd7f0f94de89ec79045174bfe7e","IPY_MODEL_624fe1c439a44ddc8621739f167a537d"],"layout":"IPY_MODEL_97557c06f55b42608e8d04fd793dc780"}},"85b7c351ee694dae86d71d4e8c267311":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8dfc8a8de06f46d39aee20de3ef6adbd","placeholder":"​","style":"IPY_MODEL_504a984424c34c5ba349a8b9f83a0b0b","value":"Downloading: 100%"}},"b1724dd7f0f94de89ec79045174bfe7e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0ccd222620f464c8e79e2541da86c83","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a311127058a5483a8bc42815a32a7eda","value":1042301}},"624fe1c439a44ddc8621739f167a537d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0adc9c0191074b6db1bd8785ce6561cc","placeholder":"​","style":"IPY_MODEL_b0b86ef24d4b443299ab0af6e1d1961c","value":" 0.99M/0.99M [00:00&lt;00:00, 1.36MB/s]"}},"97557c06f55b42608e8d04fd793dc780":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dfc8a8de06f46d39aee20de3ef6adbd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"504a984424c34c5ba349a8b9f83a0b0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0ccd222620f464c8e79e2541da86c83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a311127058a5483a8bc42815a32a7eda":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0adc9c0191074b6db1bd8785ce6561cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0b86ef24d4b443299ab0af6e1d1961c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dac297e760fb49809f6ad93cf25d75cd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1fc94e5f00ae4d90a2da71e848cb3008","IPY_MODEL_ea5a74a16ef8471bae77a686ad7fa345","IPY_MODEL_a7100dd61a6a4276af30afdfeccf4e90"],"layout":"IPY_MODEL_8e5372d40ffe44ffb34f43b24b0f47a7"}},"1fc94e5f00ae4d90a2da71e848cb3008":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_909b1e4302a84ed8a4d6c4103dcc2789","placeholder":"​","style":"IPY_MODEL_68b6677ea7c541059b9d6656497969b5","value":"Downloading: 100%"}},"ea5a74a16ef8471bae77a686ad7fa345":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a268cd2d5d004bb28b1e10aadb0ae723","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_75fcd8eecf78434da5019c788829513a","value":456318}},"a7100dd61a6a4276af30afdfeccf4e90":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f8bc2f4862244eea0e690af1ad8e779","placeholder":"​","style":"IPY_MODEL_83a7b2be009a42bc83018d9ef0f11acc","value":" 446k/446k [00:00&lt;00:00, 1.51MB/s]"}},"8e5372d40ffe44ffb34f43b24b0f47a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"909b1e4302a84ed8a4d6c4103dcc2789":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68b6677ea7c541059b9d6656497969b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a268cd2d5d004bb28b1e10aadb0ae723":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75fcd8eecf78434da5019c788829513a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0f8bc2f4862244eea0e690af1ad8e779":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83a7b2be009a42bc83018d9ef0f11acc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"815c7cd71f1d455a92c0be2318ce8afd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_60b703a6fabf4b3d8c71ace44e2ed838","IPY_MODEL_3e15d4f5130945ed9705c448fc6c1a5d","IPY_MODEL_a9c86da6e03f475c9ef2bc971683fd40"],"layout":"IPY_MODEL_03b6969be3d84f618428cc1623db5b0a"}},"60b703a6fabf4b3d8c71ace44e2ed838":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28cf021959fc4e8e8f48d78a67362500","placeholder":"​","style":"IPY_MODEL_45fcea7b6aea4b449d7e403270ea8018","value":"Downloading: 100%"}},"3e15d4f5130945ed9705c448fc6c1a5d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6cd0ba666ec14420aa27aa516423860a","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_33a3fd7028164dc598b3c0602ac97f85","value":1355256}},"a9c86da6e03f475c9ef2bc971683fd40":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_381f44f9a49c49559da3b821f1e9c200","placeholder":"​","style":"IPY_MODEL_3d7afe6d70e947a18241a4c7ee8c75c8","value":" 1.29M/1.29M [00:00&lt;00:00, 2.89MB/s]"}},"03b6969be3d84f618428cc1623db5b0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28cf021959fc4e8e8f48d78a67362500":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45fcea7b6aea4b449d7e403270ea8018":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6cd0ba666ec14420aa27aa516423860a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33a3fd7028164dc598b3c0602ac97f85":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"381f44f9a49c49559da3b821f1e9c200":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d7afe6d70e947a18241a4c7ee8c75c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c8e5a8113aa64514b4fb2ff359faeb5a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3bd5f802b9db43d59eb08e8ba14e933d","IPY_MODEL_8970668192e24ea6ad732dac342715d4","IPY_MODEL_bfb05d8eed6740d18782968d821f975a"],"layout":"IPY_MODEL_da90394b4e0b4a63bc6afc98eb835e7c"}},"3bd5f802b9db43d59eb08e8ba14e933d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82d222e71505405bbaf5204e22aabe3d","placeholder":"​","style":"IPY_MODEL_1854f3cc04c445c6bd5b743a960921a9","value":"Downloading: 100%"}},"8970668192e24ea6ad732dac342715d4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_081577bed22e4a5b91ea0b6b82e5ce23","max":548118077,"min":0,"orientation":"horizontal","style":"IPY_MODEL_136d4ed77a504830806419e023d3d84f","value":548118077}},"bfb05d8eed6740d18782968d821f975a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a0d553f64be47858f678194c261c93f","placeholder":"​","style":"IPY_MODEL_62773a65403c45ea8c467bcf5005db3b","value":" 523M/523M [00:10&lt;00:00, 59.6MB/s]"}},"da90394b4e0b4a63bc6afc98eb835e7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82d222e71505405bbaf5204e22aabe3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1854f3cc04c445c6bd5b743a960921a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"081577bed22e4a5b91ea0b6b82e5ce23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"136d4ed77a504830806419e023d3d84f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8a0d553f64be47858f678194c261c93f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62773a65403c45ea8c467bcf5005db3b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["参考：https://towardsdatascience.com/conditional-text-generation-by-fine-tuning-gpt-2-11c1a9fc639d"],"metadata":{"id":"q6J4_-aqPYpD"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"umetX8s0Mkk4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657203851588,"user_tz":-540,"elapsed":20734,"user":{"displayName":"brain colorful","userId":"11385568926180024860"}},"outputId":"9e8f75fc-d48e-4e13-c9aa-cb850db84236"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install git+https://github.com/huggingface/transformers  # ソースコードから直接transformersをインストール\n","!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HGcO0_v44bZf","executionInfo":{"status":"ok","timestamp":1657203888645,"user_tz":-540,"elapsed":37066,"user":{"displayName":"brain colorful","userId":"11385568926180024860"}},"outputId":"6a0d9716-8027-4cbf-f7be-003c79f7b236"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-wkeirgst\n","  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-wkeirgst\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (2022.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (3.7.1)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 11.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (1.21.6)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 53.6 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (4.11.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (4.64.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.0.dev0) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.21.0.dev0) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.21.0.dev0) (3.8.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.0.dev0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.0.dev0) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.0.dev0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.0.dev0) (2.10)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.21.0.dev0-py3-none-any.whl size=4584376 sha256=464b11172cd13326ea66d1ab5398f073a2b4b53e6a4a5bad9ee3e1866197e375\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ykw5i_pg/wheels/35/2e/a7/d819e3310040329f0f47e57c9e3e7a7338aa5e74c49acfe522\n","Successfully built transformers\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.0.dev0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.3.2-py3-none-any.whl (362 kB)\n","\u001b[K     |████████████████████████████████| 362 kB 5.0 MB/s \n","\u001b[?25hCollecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 58.8 MB/s \n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 71.7 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 73.0 MB/s \n","\u001b[?25hRequirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.8.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.4)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 69.4 MB/s \n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 63.9 MB/s \n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 5.0 MB/s \n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 76.6 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n","Collecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.3.2 frozenlist-1.3.0 fsspec-2022.5.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/huggingface/transformers\n","\n","# run_clm.pyを使う\n","!ls ./transformers/examples/pytorch/language-modeling/\n","# README.md   run_clm_no_trainer.py  run_mlm_no_trainer.py  run_plm.py\n","# requirements.txt  run_clm.py       run_mlm.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tecZoGwe4dIB","executionInfo":{"status":"ok","timestamp":1657203902351,"user_tz":-540,"elapsed":13715,"user":{"displayName":"brain colorful","userId":"11385568926180024860"}},"outputId":"6bf1e19b-2e1f-4848-e358-aedd6b9a32fe"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'transformers'...\n","remote: Enumerating objects: 100221, done.\u001b[K\n","remote: Counting objects: 100% (161/161), done.\u001b[K\n","remote: Compressing objects: 100% (107/107), done.\u001b[K\n","remote: Total 100221 (delta 61), reused 108 (delta 45), pack-reused 100060\u001b[K\n","Receiving objects: 100% (100221/100221), 93.69 MiB | 26.78 MiB/s, done.\n","Resolving deltas: 100% (73860/73860), done.\n","README.md\t  run_clm_no_trainer.py  run_mlm_no_trainer.py\trun_plm.py\n","requirements.txt  run_clm.py\t\t run_mlm.py\n"]}]},{"cell_type":"code","source":["MODEL = \"gpt2\"\n","OUTPUT_DIR = \"drive/MyDrive/colab/output_gpt2/\"\n","DATA_PATH = \"drive/MyDrive/colab/data/rocstories.txt\"\n","DATA_JOINED_PATH = \"drive/MyDrive/colab/data/data_joined.txt\"\n","SPECIAL_TOKENS = { \"bos_token\": \"<|BOS|>\",\n","                   \"eos_token\": \"<|EOS|>\",\n","                   \"unk_token\": \"<|UNK|>\",                    \n","                   \"pad_token\": \"<|PAD|>\",\n","                   \"sep_token\": \"<|SEP|>\"}"],"metadata":{"id":"UF5DL0lEdw7A","executionInfo":{"status":"ok","timestamp":1657203902352,"user_tz":-540,"elapsed":23,"user":{"displayName":"brain colorful","userId":"11385568926180024860"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n","# AutoModelForPreTraining\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","tokenizer.add_special_tokens(SPECIAL_TOKENS)\n","tokenizer.do_lower_case = True  # due to some bug of tokenizer config loading\n","\n","config = AutoConfig.from_pretrained(MODEL,\n","                                    bos_token_id=tokenizer.bos_token_id,\n","                                    eos_token_id=tokenizer.eos_token_id,\n","                                    sep_token_id=tokenizer.sep_token_id,\n","                                    pad_token_id=tokenizer.pad_token_id,\n","                                    output_hidden_states=False)\n","model = AutoModelForCausalLM.from_pretrained(MODEL, config=config)\n","#Special tokens added, model needs to be resized accordingly\n","model.resize_token_embeddings(len(tokenizer))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194,"referenced_widgets":["ddf5687c4b6c4c72b301f9a524244012","47ef2c820ef64d609d44c4d6dadf4423","2765c37a923143bea2cccfa5591f80d9","cd6287e886684f4a81ad47c9a3b4fd26","d5b96c1c7554417683894ebe862fffc7","927d4d95519e4d42a2f6eaa2cf38e344","b992f7b118404e479de1294984434619","d3b082b4521e4cbd851a0ca9df585e80","e615d5805e96415fa2f0c6ade6cc59d0","fab742fd8f154d9db9e8282af795af97","79345e556f2e412b9e25efa72fbd30c7","34a146a132e34475bc20442d02240c78","85b7c351ee694dae86d71d4e8c267311","b1724dd7f0f94de89ec79045174bfe7e","624fe1c439a44ddc8621739f167a537d","97557c06f55b42608e8d04fd793dc780","8dfc8a8de06f46d39aee20de3ef6adbd","504a984424c34c5ba349a8b9f83a0b0b","c0ccd222620f464c8e79e2541da86c83","a311127058a5483a8bc42815a32a7eda","0adc9c0191074b6db1bd8785ce6561cc","b0b86ef24d4b443299ab0af6e1d1961c","dac297e760fb49809f6ad93cf25d75cd","1fc94e5f00ae4d90a2da71e848cb3008","ea5a74a16ef8471bae77a686ad7fa345","a7100dd61a6a4276af30afdfeccf4e90","8e5372d40ffe44ffb34f43b24b0f47a7","909b1e4302a84ed8a4d6c4103dcc2789","68b6677ea7c541059b9d6656497969b5","a268cd2d5d004bb28b1e10aadb0ae723","75fcd8eecf78434da5019c788829513a","0f8bc2f4862244eea0e690af1ad8e779","83a7b2be009a42bc83018d9ef0f11acc","815c7cd71f1d455a92c0be2318ce8afd","60b703a6fabf4b3d8c71ace44e2ed838","3e15d4f5130945ed9705c448fc6c1a5d","a9c86da6e03f475c9ef2bc971683fd40","03b6969be3d84f618428cc1623db5b0a","28cf021959fc4e8e8f48d78a67362500","45fcea7b6aea4b449d7e403270ea8018","6cd0ba666ec14420aa27aa516423860a","33a3fd7028164dc598b3c0602ac97f85","381f44f9a49c49559da3b821f1e9c200","3d7afe6d70e947a18241a4c7ee8c75c8","c8e5a8113aa64514b4fb2ff359faeb5a","3bd5f802b9db43d59eb08e8ba14e933d","8970668192e24ea6ad732dac342715d4","bfb05d8eed6740d18782968d821f975a","da90394b4e0b4a63bc6afc98eb835e7c","82d222e71505405bbaf5204e22aabe3d","1854f3cc04c445c6bd5b743a960921a9","081577bed22e4a5b91ea0b6b82e5ce23","136d4ed77a504830806419e023d3d84f","8a0d553f64be47858f678194c261c93f","62773a65403c45ea8c467bcf5005db3b"]},"id":"JtP-7W1z4xLs","executionInfo":{"status":"ok","timestamp":1657203922044,"user_tz":-540,"elapsed":19713,"user":{"displayName":"brain colorful","userId":"11385568926180024860"}},"outputId":"6d65a643-c37b-4167-f74d-aeab39226846"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddf5687c4b6c4c72b301f9a524244012"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34a146a132e34475bc20442d02240c78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dac297e760fb49809f6ad93cf25d75cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"815c7cd71f1d455a92c0be2318ce8afd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/523M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8e5a8113aa64514b4fb2ff359faeb5a"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["Embedding(50262, 768)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["import random\n","\n","data_list = []\n","sent = \"\"\n","\n","with open(DATA_PATH, 'r', encoding=\"shift-jis\") as f:\n","  data_list = f.readlines()\n","\n","with open(DATA_JOINED_PATH, 'w', encoding=\"utf-8\") as f:  \n","  for data in data_list:\n","    pieces = data.split()\n","    sent = SPECIAL_TOKENS['bos_token'] + pieces[random.randrange(len(pieces))] + SPECIAL_TOKENS['sep_token'] + \" \".join(pieces).replace(\"\\n\", \"\") + SPECIAL_TOKENS['eos_token']\n","    f.write(sent + \"\\n\")"],"metadata":{"id":"CXIEwt8yczHd","executionInfo":{"status":"ok","timestamp":1657203922425,"user_tz":-540,"elapsed":403,"user":{"displayName":"brain colorful","userId":"11385568926180024860"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["print(sent)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iBO3QS4gc6zU","executionInfo":{"status":"ok","timestamp":1657203922426,"user_tz":-540,"elapsed":12,"user":{"displayName":"brain colorful","userId":"11385568926180024860"}},"outputId":"55ab9413-4c13-4a98-9560-efcfe7d3be9d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["<|BOS|>me<|SEP|>My parents gave me enough money to buy the console.<|EOS|>\n"]}]},{"cell_type":"code","source":["!python ./transformers/examples/pytorch/language-modeling/run_clm.py \\\n","    --model_name_or_path=gpt2 \\\n","    --train_file=drive/MyDrive/colab/data/data_joined.txt \\\n","    --validation_file=drive/MyDrive/colab/data/data_joined.txt \\\n","    --per_device_train_batch_size=1 \\\n","    --per_device_eval_batch_size=1 \\\n","    --do_train \\\n","    --do_eval \\\n","    --output_dir=drive/MyDrive/colab/output_gpt2/ \\\n","    --num_train_epochs=5 \\\n","    --save_total_limit=3 \\\n","    --save_steps=10000 \\\n","    --overwrite_output_dir\n","\n","\"\"\"\n","    --model_name_or_path=gpt2 \\\n","    --train_file=drive/MyDrive/colab/data/data_joined.txt \\\n","    --validation_file=drive/MyDrive/colab/data/data_joined.txt \\\n","    --do_train \\\n","    --do_eval \\\n","    --num_train_epochs=5 \\\n","    --save_total_limit=3 \\\n","    --save_steps=10000 \\\n","    --per_device_train_batch_size=1 \\\n","    --per_device_eval_batch_size=1 \\\n","    --output_dir=drive/MyDrive/colab/output_gpt2/ \\\n","    --use_fast_tokenizer=False \\\n","    --overwrite_output_dir\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"OkiNK_gtdA3x","executionInfo":{"status":"ok","timestamp":1657204461752,"user_tz":-540,"elapsed":539038,"user":{"displayName":"brain colorful","userId":"11385568926180024860"}},"outputId":"bd3e3a2c-8681-4053-98ae-f7f9166d52e3"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["07/07/2022 14:25:26 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","07/07/2022 14:25:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_min_num_params=0,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=drive/MyDrive/colab/output_gpt2/runs/Jul07_14-25-26_ef7fd3635b94,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=5.0,\n","optim=adamw_hf,\n","output_dir=drive/MyDrive/colab/output_gpt2/,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=1,\n","per_device_train_batch_size=1,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=drive/MyDrive/colab/output_gpt2/,\n","save_on_each_node=False,\n","save_steps=10000,\n","save_strategy=steps,\n","save_total_limit=3,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","07/07/2022 14:25:26 - WARNING - datasets.builder - Using custom data configuration default-287a91f70db3119e\n","07/07/2022 14:25:26 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-287a91f70db3119e/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08)\n","Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-287a91f70db3119e/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08...\n","Downloading data files: 100% 2/2 [00:00<00:00, 3116.12it/s]\n","07/07/2022 14:25:26 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n","07/07/2022 14:25:26 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n","Extracting data files: 100% 2/2 [00:00<00:00, 121.86it/s]\n","07/07/2022 14:25:26 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n","07/07/2022 14:25:26 - INFO - datasets.builder - Generating train split\n","07/07/2022 14:25:26 - INFO - datasets.builder - Generating validation split\n","07/07/2022 14:25:26 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-287a91f70db3119e/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08. Subsequent calls will reuse this data.\n","100% 2/2 [00:00<00:00, 404.56it/s]\n","[INFO|configuration_utils.py:664] 2022-07-07 14:25:26,962 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n","[INFO|configuration_utils.py:713] 2022-07-07 14:25:26,962 >> Model config GPT2Config {\n","  \"_name_or_path\": \"gpt2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.21.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","[INFO|tokenization_auto.py:396] 2022-07-07 14:25:27,093 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:664] 2022-07-07 14:25:27,229 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n","[INFO|configuration_utils.py:713] 2022-07-07 14:25:27,230 >> Model config GPT2Config {\n","  \"_name_or_path\": \"gpt2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.21.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","[INFO|tokenization_utils_base.py:1801] 2022-07-07 14:25:28,171 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n","[INFO|tokenization_utils_base.py:1801] 2022-07-07 14:25:28,171 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|tokenization_utils_base.py:1801] 2022-07-07 14:25:28,171 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n","[INFO|tokenization_utils_base.py:1801] 2022-07-07 14:25:28,171 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1801] 2022-07-07 14:25:28,171 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1801] 2022-07-07 14:25:28,171 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:664] 2022-07-07 14:25:28,301 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n","[INFO|configuration_utils.py:713] 2022-07-07 14:25:28,301 >> Model config GPT2Config {\n","  \"_name_or_path\": \"gpt2\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.21.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50257\n","}\n","\n","[INFO|modeling_utils.py:1999] 2022-07-07 14:25:28,536 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n","[INFO|modeling_utils.py:2389] 2022-07-07 14:25:30,307 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","[INFO|modeling_utils.py:2398] 2022-07-07 14:25:30,307 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","07/07/2022 14:25:30 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7effd2a92b90> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","Running tokenizer on dataset:   0% 0/8 [00:00<?, ?ba/s]07/07/2022 14:25:30 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-287a91f70db3119e/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08/cache-1c80317fa3b1799d.arrow\n","Running tokenizer on dataset: 100% 8/8 [00:00<00:00, 16.97ba/s]\n","07/07/2022 14:25:30 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7effd290cef0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n","Running tokenizer on dataset:   0% 0/8 [00:00<?, ?ba/s]07/07/2022 14:25:30 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-287a91f70db3119e/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08/cache-bdd640fb06671ad1.arrow\n","Running tokenizer on dataset: 100% 8/8 [00:00<00:00, 17.05ba/s]\n","07/07/2022 14:25:31 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.group_texts at 0x7effd2b6b320> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n","Grouping texts in chunks of 1024:   0% 0/8 [00:00<?, ?ba/s]07/07/2022 14:25:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-287a91f70db3119e/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08/cache-3eb13b9046685257.arrow\n","Grouping texts in chunks of 1024: 100% 8/8 [00:00<00:00, 31.99ba/s]\n","07/07/2022 14:25:31 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.group_texts at 0x7effd2a97290> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n","Grouping texts in chunks of 1024:   0% 0/8 [00:00<?, ?ba/s]07/07/2022 14:25:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-287a91f70db3119e/0.0.0/acc32f2f2ef863c93c2f30c52f7df6cc9053a1c2230b8d7da0d210404683ca08/cache-23b8c1e9392456de.arrow\n","Grouping texts in chunks of 1024: 100% 8/8 [00:00<00:00, 31.11ba/s]\n","07/07/2022 14:25:32 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.3.2/metrics/accuracy/accuracy.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp2rno7ddy\n","Downloading builder script: 4.21kB [00:00, 3.28MB/s]       \n","07/07/2022 14:25:32 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.3.2/metrics/accuracy/accuracy.py in cache at /root/.cache/huggingface/datasets/downloads/1834697c34339eaba643e589f504558a0eaa725a7f7595702b5bf31bf9b3753f.32b3507481ea2e26fd6a2b34c9976e9da377302faaf35089eb1cd971d41bb0ff.py\n","07/07/2022 14:25:32 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/1834697c34339eaba643e589f504558a0eaa725a7f7595702b5bf31bf9b3753f.32b3507481ea2e26fd6a2b34c9976e9da377302faaf35089eb1cd971d41bb0ff.py\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","[INFO|trainer.py:1522] 2022-07-07 14:25:51,264 >> ***** Running training *****\n","[INFO|trainer.py:1523] 2022-07-07 14:25:51,264 >>   Num examples = 220\n","[INFO|trainer.py:1524] 2022-07-07 14:25:51,264 >>   Num Epochs = 5\n","[INFO|trainer.py:1525] 2022-07-07 14:25:51,265 >>   Instantaneous batch size per device = 1\n","[INFO|trainer.py:1526] 2022-07-07 14:25:51,265 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n","[INFO|trainer.py:1527] 2022-07-07 14:25:51,265 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1528] 2022-07-07 14:25:51,265 >>   Total optimization steps = 1100\n","{'loss': 1.2339, 'learning_rate': 2.7272727272727273e-05, 'epoch': 2.27}\n","{'loss': 1.0407, 'learning_rate': 4.5454545454545455e-06, 'epoch': 4.55}\n","100% 1100/1100 [07:43<00:00,  2.35it/s][INFO|trainer.py:1767] 2022-07-07 14:33:34,996 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 463.7506, 'train_samples_per_second': 2.372, 'train_steps_per_second': 2.372, 'train_loss': 1.124873150912198, 'epoch': 5.0}\n","100% 1100/1100 [07:43<00:00,  2.37it/s]\n","[INFO|trainer.py:2511] 2022-07-07 14:33:35,020 >> Saving model checkpoint to drive/MyDrive/colab/output_gpt2/\n","[INFO|configuration_utils.py:451] 2022-07-07 14:33:35,173 >> Configuration saved in drive/MyDrive/colab/output_gpt2/config.json\n","[INFO|modeling_utils.py:1550] 2022-07-07 14:33:47,272 >> Model weights saved in drive/MyDrive/colab/output_gpt2/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2143] 2022-07-07 14:33:47,579 >> tokenizer config file saved in drive/MyDrive/colab/output_gpt2/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2150] 2022-07-07 14:33:47,820 >> Special tokens file saved in drive/MyDrive/colab/output_gpt2/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        5.0\n","  train_loss               =     1.1249\n","  train_runtime            = 0:07:43.75\n","  train_samples            =        220\n","  train_samples_per_second =      2.372\n","  train_steps_per_second   =      2.372\n","07/07/2022 14:33:50 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:2761] 2022-07-07 14:33:50,249 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2763] 2022-07-07 14:33:50,249 >>   Num examples = 220\n","[INFO|trainer.py:2766] 2022-07-07 14:33:50,249 >>   Batch size = 1\n","100% 220/220 [00:27<00:00,  7.72it/s]07/07/2022 14:34:18 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n","100% 220/220 [00:28<00:00,  7.74it/s]\n","***** eval metrics *****\n","  epoch                   =        5.0\n","  eval_accuracy           =     0.8048\n","  eval_loss               =     0.9205\n","  eval_runtime            = 0:00:28.59\n","  eval_samples            =        220\n","  eval_samples_per_second =      7.692\n","  eval_steps_per_second   =      7.692\n","  perplexity              =     2.5107\n","[INFO|modelcard.py:460] 2022-07-07 14:34:19,395 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8047631742646405}]}\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\n    --model_name_or_path=gpt2     --train_file=drive/MyDrive/colab/data/data_joined.txt     --validation_file=drive/MyDrive/colab/data/data_joined.txt     --do_train     --do_eval     --num_train_epochs=5     --save_total_limit=3     --save_steps=10000     --per_device_train_batch_size=1     --per_device_eval_batch_size=1     --output_dir=drive/MyDrive/colab/output_gpt2/     --use_fast_tokenizer=False     --overwrite_output_dir\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["model = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR)"],"metadata":{"id":"GxLCVZPRdKRq","executionInfo":{"status":"ok","timestamp":1657204463794,"user_tz":-540,"elapsed":2060,"user":{"displayName":"brain colorful","userId":"11385568926180024860"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","tokenizer.do_lower_case = True\n","model.to(device)\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HsqZoV20dKqo","executionInfo":{"status":"ok","timestamp":1657204474585,"user_tz":-540,"elapsed":10803,"user":{"displayName":"brain colorful","userId":"11385568926180024860"}},"outputId":"3a069924-8317-4f16-9204-282c9f842647"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50257, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",")"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["def conditional_generation(keyword, num_gen=20):\n","    input_text = SPECIAL_TOKENS['bos_token']+keyword+SPECIAL_TOKENS['sep_token']\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n","    out = model.generate(input_ids, do_sample=True, top_p=0.95, top_k=40, num_return_sequences=num_gen, max_length=50, bad_words_ids=[[1], [3]])\n","    print(\"keyword: \"+keyword)\n","    print(\"generated:\")\n","    for sent in tokenizer.batch_decode(out):\n","      sent = sent.split(SPECIAL_TOKENS['sep_token'])[1].split(SPECIAL_TOKENS['eos_token'])[0]\n","      print(sent)"],"metadata":{"id":"tDiX2TY5dRNM","executionInfo":{"status":"ok","timestamp":1657207025352,"user_tz":-540,"elapsed":215,"user":{"displayName":"brain colorful","userId":"11385568926180024860"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["conditional_generation(\"happy\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_eAG8kVUdR2f","executionInfo":{"status":"ok","timestamp":1657207028576,"user_tz":-540,"elapsed":1126,"user":{"displayName":"brain colorful","userId":"11385568926180024860"}},"outputId":"8ba853c3-bcee-46a2-c4fb-49803614485a"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["keyword: happy\n","generated:\n","Amber is happy because it is the only day her son is a year old.\n","Livie wanted a new house.\n","Joey is happy when everyone is happy.\n","Neil and Ellen were happy to meet.\n","Sandra was happy to be back home.\n","The happy wife said, \"I was happy last night.\n","Ben and I went to the movie together.\n","We all got to eat delicious pizza today.\n","Cameron was happy to have the chance to do something he loves.\n","He was happy to have made a change.\n","Bob was happy he had gotten some of the candy.\n","It's been a sad day for Kimmy and her family.\n","A happy, sad, happy Andrew decided to try out for the soccer team.\n","The boy felt happy to be happy and happy.\n","He always had a happy childhood.\n","He was happy to finally have found a new job.\n","Joe and Brenda were both happy that they had met.\n","Nora was nervous about the future at school.\n","Amy is happy she was able to have a baby boy.\n","They had a happy birthday with him.\n"]}]}]}